{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlW0foBVRUAZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class ThemeClassifier:\n",
        "    def __init__(self, model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"):\n",
        "        # Initialize model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(self.device, flush=True)\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Storage for theme embeddings\n",
        "        self.theme_embeddings: Dict[str, np.ndarray] = {}\n",
        "\n",
        "    def get_embedding(self, verses: List[str]) -> np.ndarray:\n",
        "      \"\"\"Generate embeddings for a list of verses using batched processing\"\"\"\n",
        "      verses_len = len(verses)\n",
        "      print(verses_len)\n",
        "      print(\"==============\")\n",
        "\n",
        "      # List to store embeddings from each batch\n",
        "      all_embeddings = []\n",
        "\n",
        "      # Initialize batch parameters\n",
        "      i = 0\n",
        "      if verses_len < 100:\n",
        "          batch_size = 10\n",
        "      else:\n",
        "          batch_size = 100\n",
        "\n",
        "      # Calculate number of full batches and whether there's a remainder\n",
        "      num_full_batches = verses_len // batch_size\n",
        "      has_remainder = verses_len % batch_size > 0\n",
        "      total_iterations = num_full_batches + (1 if has_remainder else 0)\n",
        "\n",
        "      for batch_idx in range(total_iterations):\n",
        "          # Calculate batch indices\n",
        "          start_idx = batch_idx * batch_size\n",
        "          end_idx = min(start_idx + batch_size, verses_len)\n",
        "          batch_verses = verses[start_idx:end_idx]\n",
        "\n",
        "          # Process batch\n",
        "          encoded_input = self.tokenizer(batch_verses, padding=True, truncation=True,\n",
        "                                      max_length=128, return_tensors='pt')\n",
        "          encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
        "\n",
        "          with torch.no_grad():\n",
        "              model_output = self.model(**encoded_input)\n",
        "\n",
        "          attention_mask = encoded_input['attention_mask']\n",
        "          input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n",
        "              model_output.last_hidden_state.size()).float()\n",
        "\n",
        "          # Calculate embeddings for current batch\n",
        "          batch_embeddings = torch.sum(model_output.last_hidden_state * input_mask_expanded, 1) / \\\n",
        "                            torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "          # Store batch embeddings\n",
        "          all_embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "      # Concatenate all embeddings\n",
        "      final_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "      return final_embeddings.numpy()\n",
        "\n",
        "    def process_and_save_embeddings(self, df: pd.DataFrame, save_path: str = 'content/theme_embeddings.pkl'):\n",
        "        \"\"\"Process verses and save theme embeddings to pickle file\"\"\"\n",
        "        # Process each theme\n",
        "        for theme in df['poem theme'].unique():\n",
        "            theme_verses = []\n",
        "\n",
        "            # Get all verses for this theme\n",
        "            theme_df = df[df['poem theme'] == theme]\n",
        "\n",
        "            for i, verses in enumerate(theme_df['poem verses']):\n",
        "                # Handle string or list input\n",
        "                if isinstance(verses, str):\n",
        "                    verses = eval(verses)\n",
        "                theme_verses.extend(verses)\n",
        "\n",
        "            # Generate and store embeddings for this theme\n",
        "            self.theme_embeddings[theme] = self.get_embedding(theme_verses)\n",
        "\n",
        "        # Save to pickle file\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(self.theme_embeddings, f)\n",
        "\n",
        "        return self.theme_embeddings\n",
        "\n",
        "    def load_embeddings(self, load_path: str = 'theme_embeddings.pkl'):\n",
        "        \"\"\"Load theme embeddings from pickle file\"\"\"\n",
        "        with open(load_path, 'rb') as f:\n",
        "            self.theme_embeddings = pickle.load(f)\n",
        "        return self.theme_embeddings\n",
        "\n",
        "    def classify_verse(self, verse: str, top_k: int = 1) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Classify a verse into themes based on embedding similarity\n",
        "        Returns: List of tuples (theme, confidence_score)\n",
        "        \"\"\"\n",
        "        # Generate embedding for input verse\n",
        "        verse_embedding = self.get_embedding([verse])\n",
        "\n",
        "        # Compare with each theme's verses\n",
        "        theme_scores = []\n",
        "        for theme, theme_embeddings in self.theme_embeddings.items():\n",
        "            # Calculate similarities with all verses in this theme\n",
        "            similarities = cosine_similarity(verse_embedding, theme_embeddings)[0]\n",
        "            # Use max similarity as the theme score\n",
        "            theme_score = np.max(similarities)\n",
        "            theme_scores.append((theme, theme_score))\n",
        "\n",
        "        # Sort by similarity score and return top k matches\n",
        "        return sorted(theme_scores, key=lambda x: x[1], reverse=True)[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./ashaar.csv')\n",
        "\n",
        "# Initialize classifier and process verses\n",
        "classifier = ThemeClassifier()\n",
        "\n",
        "# Save embeddings\n",
        "print(\"Processing and saving embeddings...\")\n",
        "classifier.process_and_save_embeddings(df, 'theme_embeddings.pkl')\n",
        "\n",
        "# Later, you can load the embeddings and classify new verses\n",
        "print(\"\\nLoading embeddings and classifying new verse...\")\n",
        "classifier.load_embeddings('theme_embeddings.pkl')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sVpoJ6f-SRq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify a new verse\n",
        "new_verse = \"\"\n",
        "predictions = classifier.classify_verse(new_verse, top_k=10)\n",
        "\n",
        "# Print results\n",
        "for theme, confidence in predictions:\n",
        "    print(f\"Theme: {theme}\")\n",
        "    print(f\"Confidence Score: {confidence:.4f}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "qRpNwX1WWLbl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROWyqLUwWQBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}